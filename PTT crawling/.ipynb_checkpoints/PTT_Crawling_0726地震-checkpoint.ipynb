{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬PTT地震文\n",
    "\n",
    "* 需要的資訊：\n",
    "    \n",
    "    1. 發文時間\n",
    "    2. 文章標題\n",
    "    3. 文章內容\n",
    "    4. 前100推推文內容\n",
    "    5. 發文者ID\n",
    "    6. 發文者IP位址\n",
    "    7. 該文是否屬於地震文（dummy）\n",
    "    8. 該文是否搶到地震爆文（dummy）\n",
    "   \n",
    "* 另外要寫的功能：\n",
    "\n",
    "    1. 用IP位址反查地址資訊，回傳「縣市」（str）及經緯度（某種可以被餵進google map API的格式）\n",
    "    2. 一天／一週當中PTT上站人數的pattern，輔以「當時發文者發文所處的時間」來控制error term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# 導入 模組(module) \n",
    "import requests \n",
    "# 導入 BeautifulSoup 模組(module)：解析HTML 語法工具\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from multiprocessing import Pool #https://github.com/leVirve/CrawlerTutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#八卦版文章連結\n",
    "url = \"https://www.ptt.cc/bbs/Gossiping/index.html\"\n",
    "header_over18 = {\"cookie\":\"over18=1\"}\n",
    "res = requests.get(url, headers = header_over18)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try to search keyword \"地震\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.ptt.cc/bbs/Gossiping/search?q=%E5%9C%B0%E9%9C%87 //查詢地震時的endpoint\n",
    "search_endpoint_url = 'https://www.ptt.cc/bbs/Gossiping/search'\n",
    "# 把要查詢的關鍵字參數化\n",
    "resp = requests.get(search_endpoint_url, params={'q': '地震'}, headers = header_over18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(resp.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找到全部地震文章的連結\n",
    "links = soup.find_all(\"a href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = soup.find_all(\"div\", \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[爆卦] 地震\n",
      " /bbs/Gossiping/M.1595767989.A.E49.html\n",
      "\n",
      "[爆卦] 地震\n",
      " /bbs/Gossiping/M.1595767987.A.363.html\n",
      "\n",
      "[問卦] 前幾天有下雨然後今天就地震\n",
      " /bbs/Gossiping/M.1595715228.A.863.html\n",
      "\n",
      "[爆卦] 地震分布圖\n",
      " /bbs/Gossiping/M.1595714302.A.06A.html\n",
      "\n",
      "Re: [爆卦] 地震\n",
      " /bbs/Gossiping/M.1595713380.A.212.html\n",
      "\n",
      "[爆卦] 地震\n",
      " /bbs/Gossiping/M.1595712994.A.00A.html\n",
      "\n",
      "[爆卦] 地震\n",
      " /bbs/Gossiping/M.1595712901.A.7A5.html\n",
      "\n",
      "[爆卦] 地震\n",
      " /bbs/Gossiping/M.1595712894.A.E0F.html\n",
      "\n",
      "[爆卦] 地震\n",
      " /bbs/Gossiping/M.1595712890.A.00C.html\n",
      "\n",
      "地震\n",
      " /bbs/Gossiping/M.1595712885.A.F72.html\n",
      "\n",
      "[爆卦] 地震\n",
      " /bbs/Gossiping/M.1595712861.A.047.html\n",
      "\n",
      "[問卦]解放軍用核武引發地震癱瘓台灣防空怎麼辦?\n",
      " /bbs/Gossiping/M.1595597119.A.9FA.html\n",
      "\n",
      "[問卦] 預測地震台灣國家沒人才？！\n",
      " /bbs/Gossiping/M.1595299835.A.C70.html\n",
      "\n",
      "[問卦] 關東大地震援助日本之後卻被侵略\n",
      " /bbs/Gossiping/M.1595229626.A.8A7.html\n",
      "\n",
      "[爆卦] 地震\n",
      " /bbs/Gossiping/M.1595192816.A.FA4.html\n",
      "\n",
      "[爆卦] 地震 (草屯 規模3.9)\n",
      " /bbs/Gossiping/M.1595192808.A.A19.html\n",
      "\n",
      "[新聞] 日本新型子彈列車上路 地震來也不怕\n",
      " /bbs/Gossiping/M.1594778734.A.0C6.html\n",
      "\n",
      "[新聞] 不捐了！中國地震、水患不斷 港人拒當賑\n",
      " /bbs/Gossiping/M.1594725992.A.E4D.html\n",
      "\n",
      "[問卦] 地震與墜機的關係，關於日本沉沒2020\n",
      " /bbs/Gossiping/M.1594706665.A.1BC.html\n",
      "\n",
      "Re: [新聞] 唐山爆規模5.1地震 中國官方：1976年唐山\n",
      " /bbs/Gossiping/M.1594690646.A.48D.html\n"
     ]
    }
   ],
   "source": [
    "for title in titles:\n",
    "    print(title.text, title.a[\"href\"])\n",
    "    link = \"https://www.ptt.cc/\" + title.a[\"href\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1594690646'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_id = title.a[\"href\"].split(sep = \"/\")[-1].split(sep = \".\")[1]\n",
    "article_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "# 進入搜尋頁面的第一篇文章\n",
    "link = \"https://www.ptt.cc/\" + title.a[\"href\"]\n",
    "\n",
    "# 18禁\n",
    "payload = {\n",
    "    \"from\" : \"/bbs/Gossiping/index.html\",\n",
    "    \"yes\"  : \"yes\"\n",
    "}\n",
    "rs = requests.session()\n",
    "res_subpage = rs.post(\"https://www.ptt.cc/ask/over18\", verify = False, data = payload)\n",
    "\n",
    "res_subpage = rs.get(link, verify = False)\n",
    "soup_subpage = BeautifulSoup(res_subpage.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Re: [新聞] 唐山爆規模5.1地震 中國官方：1976年唐山 - 看板 Gossiping - 批踢踢實業坊\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "批踢踢實業坊\n",
      "›\n",
      "看板 Gossiping\n",
      "關於我們\n",
      "聯絡資訊\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "返回看板\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "作者Lineage097 (狐狸壽司)看板Gossiping標題Re: [新聞] 唐山爆規模5.1地震 中國官方：1976年唐山時間Tue Jul 14 09:37:22 2020\n",
      "餘震可以賒帳幾十年也厲害了\n",
      "\n",
      "5.1級還好吧 其實不會大到房子全震光\n",
      "\n",
      "連半夜被震醒都搶不到地震文首po\n",
      "\n",
      "搶首po應該比房子被5級地震震倒還難\n",
      "-----\n",
      "Sent from JPTT on my iPhone\n",
      "\n",
      "--\n",
      "※ 發信站: 批踢踢實業坊(ptt.cc), 來自: 223.138.17.98 (臺灣)\n",
      "※ 文章網址: https://www.ptt.cc/bbs/Gossiping/M.1594690646.A.48D.html\n",
      "噓 flavorBZ: 對於幾十億年的地球來說，幾十年算瞬間。同意嗎？ 07/14 09:38\n",
      "→ d8917936: 有沒有可能是2.5億年前隕石撞擊的餘震 07/14 09:39\n",
      "推 zz71: 一樓文組韓粉不意外 07/14 09:41\n",
      "推 coupon: 支那腦黨說是就是 07/14 10:30\n",
      "\n",
      "\n",
      "本網站已依台灣網站內容分級規定處理。此區域為限制級，未滿十八歲者不得瀏覽。\n",
      "\n",
      "\n",
      "  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n",
      "  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n",
      "  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n",
      "  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n",
      "\n",
      "  ga('create', 'UA-32365737-1', {\n",
      "    cookieDomain: 'ptt.cc',\n",
      "    legacyCookieDomain: 'ptt.cc'\n",
      "  });\n",
      "  ga('send', 'pageview');\n",
      "\n",
      "\n",
      "\n",
      "(function(){window['__CF$cv$params']={r:'5b8e466f58d4d631',m:'e2c4feb13734ecd56130bf5119d554face20d7ff-1595768128-1800-ARMQg6Z7hm/S7nbDyqND2scDg9LbWxXHXAat3RjYG9tRmDgjzQBmJNz1sRknn5mZwUkNgnFxzVMEdryerYQltZ/PvZSv7hQe8uL/yodJb5LzVaErZfYEE1D1HHLP5SmBhC7BnVOpam6PD02hINDQHcIaA/jZAn8xdBE3HIW3eGcX',s:[0x410bbb81f9,0x66d926ed34],}})();\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup_subpage.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<span class=\"article-meta-value\">Lineage097 (狐狸壽司)</span>, <span class=\"article-meta-value\">Gossiping</span>, <span class=\"article-meta-value\">Re: [新聞] 唐山爆規模5.1地震 中國官方：1976年唐山</span>, <span class=\"article-meta-value\">Tue Jul 14 09:37:22 2020</span>]\n"
     ]
    }
   ],
   "source": [
    "#一篇文章的 作者、標題、時間、看板\n",
    "header = soup_subpage.find_all(\"span\", \"article-meta-value\")\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lineage097 (狐狸壽司)\n",
      "Gossiping\n",
      "Re: [新聞] 唐山爆規模5.1地震 中國官方：1976年唐山\n",
      "Tue Jul 14 09:37:22 2020\n"
     ]
    }
   ],
   "source": [
    "print(header[0].text) #author\n",
    "print(header[1].text) #board\n",
    "print(header[2].text) #title\n",
    "print(header[3].text) #date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "作者Lineage097 (狐狸壽司)看板Gossiping標題Re: [新聞] 唐山爆規模5.1地震 中國官方：1976年唐山時間Tue Jul 14 09:37:22 2020\n",
      "餘震可以賒帳幾十年也厲害了\n",
      "\n",
      "5.1級還好吧 其實不會大到房子全震光\n",
      "\n",
      "連半夜被震醒都搶不到地震文首po\n",
      "\n",
      "搶首po應該比房子被5級地震震倒還難\n",
      "-----\n",
      "Sent from JPTT on my iPhone\n",
      "\n",
      "--\n",
      "※ 發信站: 批踢踢實業坊(ptt.cc), 來自: 223.138.17.98 (臺灣)\n",
      "※ 文章網址: https://www.ptt.cc/bbs/Gossiping/M.1594690646.A.48D.html\n",
      "噓 flavorBZ: 對於幾十億年的地球來說，幾十年算瞬間。同意嗎？ 07/14 09:38\n",
      "→ d8917936: 有沒有可能是2.5億年前隕石撞擊的餘震 07/14 09:39\n",
      "推 zz71: 一樓文組韓粉不意外 07/14 09:41\n",
      "推 coupon: 支那腦黨說是就是 07/14 10:30\n",
      "\n",
      "\n",
      "本網站已依台灣網站內容分級規定處理。此區域為限制級，未滿十八歲者不得瀏覽。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_text = soup_subpage.find(id = \"main-container\").text\n",
    "print(main_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['噓 flavorBZ: 對於幾十億年的地球來說，幾十年算瞬間。同意嗎？ 07/14 09:38',\n",
       " '→ d8917936: 有沒有可能是2.5億年前隕石撞擊的餘震 07/14 09:39',\n",
       " '推 zz71: 一樓文組韓粉不意外 07/14 09:41',\n",
       " '推 coupon: 支那腦黨說是就是 07/14 10:30',\n",
       " '',\n",
       " '',\n",
       " '本網站已依台灣網站內容分級規定處理。此區域為限制級，未滿十八歲者不得瀏覽。',\n",
       " '']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#推文\n",
    "main_text.split(sep = \"html\")[1].split(sep = \"\\n\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "內容：餘震可以賒帳幾十年也厲害了\n",
      "\n",
      "5.1級還好吧 其實不會大到房子全震光\n",
      "\n",
      "連半夜被震醒都搶不到地震文首po\n",
      "\n",
      "搶首po應該比房子被5級地震震倒還難\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 把整個內容切割透過 \"-- \" 切割成2個陣列\n",
    "pre_text = main_text.split('--')[0]\n",
    "    \n",
    "# 把每段文字 根據 '\\n' 切開\n",
    "texts = pre_text.split('\\n')\n",
    "# 如果你爬多篇你會發現 \n",
    "contents = texts[2:]\n",
    "# 內容\n",
    "content = '\\n'.join(contents)\n",
    "\n",
    "print('內容：'+content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 223.138.17.98 (臺灣)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IP_address = main_text.split('來自:')[1].split(\"\\n\")[0]\n",
    "IP_address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "f\"{article_id}\" = {\n",
    "    \"Date\" : \"\"\n",
    "    \"Title\" : \"\"\n",
    "    \"Author\" : \"\"\n",
    "    \"IP\" : \"\"\n",
    "    \"Content\" : \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1594690646'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{article_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M1594690646'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec_str = \"M\" + f\"{article_id}\"\n",
    "exec_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(exec_str+\" = {}\", globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(f\"{exec_str}\", globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 實作：把搜尋頁第一頁的文章都爬下來，存在一個dict內"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.ptt.cc/bbs/Gossiping/search?q=%E5%9C%B0%E9%9C%87 //查詢地震時的endpoint\n",
    "#search_endpoint_url = 'https://www.ptt.cc/bbs/Gossiping/search'\n",
    "# 把要查詢的關鍵字參數化\n",
    "#resp = requests.get(search_endpoint_url, params={'q': '地震'}, headers = header_over18)\n",
    "\n",
    "#soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "#忽略warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 建立字典\n",
    "earthquake_dict = {} # key是文章的link，value是另一本以文章的unique link為名的dict\n",
    "# list內則有：發文時間、文章標題、作者ID、作者IP、文章內容\n",
    "names = globals()\n",
    "\n",
    "# 搜尋頁面換頁\n",
    "# https://www.ptt.cc/bbs/Gossiping/search?page= 3 &q=%E5%9C%B0%E9%9C%87\n",
    "# key換成index\n",
    "index = 1\n",
    "for page_count in range(1, 52+1):\n",
    "    #search_page_url = \"https://www.ptt.cc/bbs/Gossiping/search?page=\" + str(page_count) + \"&q=%E5%9C%B0%E9%9C%87\"\n",
    "    search_page_url = \"https://www.ptt.cc/bbs/Gossiping/search?page=\" + str(page_count) + \"&q=震\"\n",
    "    resp = requests.get(search_page_url, headers = header_over18)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    \n",
    "    # 找到該搜尋頁面下 全部地震文章的連結\n",
    "    titles = soup.find_all(\"div\", \"title\") #找到每篇文章的標題及連結\n",
    "    for title in titles:\n",
    "        #print(title.text, title.a[\"href\"])\n",
    "        \n",
    "        link = \"https://www.ptt.cc/\" + title.a[\"href\"]\n",
    "        article_id = title.a[\"href\"].split(sep = \"/\")[-1].split(sep = \".\")[1]\n",
    "        article_id = index\n",
    "        index+=1\n",
    "        \n",
    "        #為這篇文章建一個dictionary\n",
    "        name_str = \"M\" + f\"{article_id}\"\n",
    "        names[\"M_%s\" %article_id] = {}\n",
    "        earthquake_dict[name_str] = names[\"M_%s\" %article_id]\n",
    "\n",
    "        \n",
    "        # 進入搜尋頁面的第一篇文章\n",
    "        payload = {\n",
    "            \"from\" : \"/bbs/Gossiping/index.html\",\n",
    "            \"yes\"  : \"yes\"\n",
    "        }\n",
    "        rs = requests.session()\n",
    "        res_subpage = rs.post(\"https://www.ptt.cc/ask/over18\", verify = False, data = payload)\n",
    "        res_subpage = rs.get(link, verify = False)\n",
    "        soup_subpage = BeautifulSoup(res_subpage.text, \"html.parser\")\n",
    "\n",
    "        #一篇文章的 作者、標題、時間、看板\n",
    "        header = soup_subpage.find_all(\"span\", \"article-meta-value\")\n",
    "        #print(header)\n",
    "        #print(header[0].text) #author\n",
    "        #print(header[1].text) #board\n",
    "        #print(header[2].text) #title\n",
    "        #print(header[3].text) #date\n",
    "        \n",
    "        try: #如果有「看板」的話\n",
    "            names[\"M_%s\" %article_id][\"Author\"] = header[0].text\n",
    "            names[\"M_%s\" %article_id][\"Board\"] = header[1].text\n",
    "            names[\"M_%s\" %article_id][\"Title\"] = header[2].text\n",
    "            names[\"M_%s\" %article_id][\"Date\"] = header[3].text\n",
    "        except:\n",
    "            try: #如果是回覆文章，沒有「看板」的話\n",
    "                names[\"M_%s\" %article_id][\"Author\"] = header[0].text\n",
    "                names[\"M_%s\" %article_id][\"Title\"] = header[1].text\n",
    "                names[\"M_%s\" %article_id][\"Date\"] = header[2].text\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # 內文\n",
    "        try:\n",
    "            main_text = soup_subpage.find(id = \"main-container\").text\n",
    "        except:\n",
    "            pass\n",
    "        #print(main_text)\n",
    "        # 把整個內容切割透過 \"-- \" 切割成2個陣列\n",
    "        pre_text = main_text.split('--')[0]\n",
    "        # 把每段文字 根據 '\\n' 切開\n",
    "        texts = pre_text.split('\\n')\n",
    "        # 如果你爬多篇你會發現 \n",
    "        contents = texts[2:]\n",
    "        # 內容\n",
    "        content = '\\n'.join(contents)\n",
    "        #print('內容：'+content)\n",
    "        names[\"M_%s\" %article_id][\"Content\"] = content\n",
    "\n",
    "        \n",
    "        #IP位址\n",
    "        try:\n",
    "            try:\n",
    "                IP_address = main_text.split('來自:')[1].split(\"\\n\")[0]\n",
    "                #print(IP_address)\n",
    "                names[\"M_%s\" %article_id][\"Full IP\"] = IP_address\n",
    "            except:\n",
    "                try:\n",
    "                    author = header[0].text\n",
    "                    authorID = author.split(sep = \" (\")[0]\n",
    "                    #print(main_text.split(sep = authorID)[2].split(sep = \"),\")[0].split()[0][1:])\n",
    "                    IP_address = main_text.split(sep = authorID)[2].split(sep = \"),\")[0].split()[0][1:]\n",
    "                    names[\"M_%s\" %article_id][\"Full IP\"] = IP_address\n",
    "                except:\n",
    "                    pass\n",
    "        except:\n",
    "            names[\"M_%s\" %article_id][\"Full IP\"] = \"Not Found\"\n",
    "        \n",
    "        #只有數字的IP\n",
    "        try:\n",
    "            IP_shorten = IP_address.split(sep = \" \")[1]\n",
    "            names[\"M_%s\" %article_id][\"IP\"] = IP_shorten\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # 推文\n",
    "        try:            \n",
    "            push = main_text.split(sep = \"html\")[1].split(sep = \"\\n\")[1:]\n",
    "            names[\"M_%s\" %article_id][\"Push\"] = push\n",
    "        except:\n",
    "            names[\"M_%s\" %article_id][\"Push\"] = \"No Push\"\n",
    "        \n",
    "        time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author = header[0].text\n",
    "authorID = author.split(sep = \" (\")[0]\n",
    "#print(main_text.split(sep = authorID)[2].split(sep = \"),\")[0].split()[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(earthquake_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_dict[\"M3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IP_shorten = IP_address.split(sep = \" \")[1]\n",
    "IP_shorten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "np.save(\"earthquake_dict_0726地震_2\", earthquake_dict)\n",
    "\n",
    "# Load\n",
    "#read_dictionary = np.load('my_file.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_dict[\"M20\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
