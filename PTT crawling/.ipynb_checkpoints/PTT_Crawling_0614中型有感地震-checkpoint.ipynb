{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬PTT地震文\n",
    "\n",
    "* 需要的資訊：\n",
    "    \n",
    "    1. 發文時間\n",
    "    2. 文章標題\n",
    "    3. 文章內容\n",
    "    4. 前100推推文內容\n",
    "    5. 發文者ID\n",
    "    6. 發文者IP位址\n",
    "    7. 該文是否屬於地震文（dummy）\n",
    "    8. 該文是否搶到地震爆文（dummy）\n",
    "   \n",
    "* 另外要寫的功能：\n",
    "\n",
    "    1. 用IP位址反查地址資訊，回傳「縣市」（str）及經緯度（某種可以被餵進google map API的格式）\n",
    "    2. 一天／一週當中PTT上站人數的pattern，輔以「當時發文者發文所處的時間」來控制error term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.25.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# 導入 模組(module) \n",
    "import requests \n",
    "# 導入 BeautifulSoup 模組(module)：解析HTML 語法工具\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from multiprocessing import Pool #https://github.com/leVirve/CrawlerTutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#八卦版文章連結\n",
    "url = \"https://www.ptt.cc/bbs/Gossiping/index.html\"\n",
    "header_over18 = {\"cookie\":\"over18=1\"}\n",
    "res = requests.get(url, headers = header_over18)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try to search keyword \"地震\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.ptt.cc/bbs/Gossiping/search?q=%E5%9C%B0%E9%9C%87 //查詢地震時的endpoint\n",
    "search_endpoint_url = 'https://www.ptt.cc/bbs/Gossiping/search'\n",
    "# 把要查詢的關鍵字參數化\n",
    "resp = requests.get(search_endpoint_url, params={'q': '震'}, headers = header_over18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(resp.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找到全部地震文章的連結\n",
    "links = soup.find_all(\"a href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = soup.find_all(\"div\", \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[問卦] 為什麼地震要叫地牛翻身？\n",
      " /bbs/Gossiping/M.1592081630.A.05E.html\n",
      "\n",
      "[問卦] 地震先收到簡訊可以幹麻？\n",
      " /bbs/Gossiping/M.1592080590.A.C3A.html\n",
      "\n",
      "[問卦] 地震以為是自己體重怎麼辦\n",
      " /bbs/Gossiping/M.1592080586.A.905.html\n",
      "\n",
      "[問卦] 地震的時候沒有雅筑能告白？\n",
      " /bbs/Gossiping/M.1592080332.A.10D.html\n",
      "\n",
      "[問卦] 震央在海裡時海中的魚在想什麼\n",
      " /bbs/Gossiping/M.1592080154.A.9A7.html\n",
      "\n",
      "[爆卦] 花蓮外海04:18出現6.3地震，逸涵我愛妳\n",
      " /bbs/Gossiping/M.1592080031.A.78C.html\n",
      "\n",
      "[問卦] 為什麼地震不大也要發國家級警報？\n",
      " /bbs/Gossiping/M.1592079934.A.CA5.html\n",
      "\n",
      "[問卦] 哪裡有地震\n",
      " /bbs/Gossiping/M.1592079879.A.F19.html\n",
      "\n",
      "Re: [爆卦] 地震\n",
      " /bbs/Gossiping/M.1592079818.A.A6F.html\n",
      "\n",
      "[問卦] 為何是警報先來 地震過5秒才搖\n",
      " /bbs/Gossiping/M.1592079792.A.F3D.html\n",
      "\n",
      "地震\n",
      " /bbs/Gossiping/M.1592079700.A.B9F.html\n",
      "\n",
      "[爆卦] 地震\n",
      " /bbs/Gossiping/M.1592079689.A.1DA.html\n",
      "\n",
      "[爆卦] 地震 \n",
      " /bbs/Gossiping/M.1592079680.A.393.html\n",
      "\n",
      "[問卦] 地震\n",
      " /bbs/Gossiping/M.1592079675.A.FF8.html\n",
      "\n",
      "[爆卦] 幹你娘地震 好大\n",
      " /bbs/Gossiping/M.1592079663.A.FFA.html\n",
      "\n",
      "[爆卦] 震\n",
      " /bbs/Gossiping/M.1592079663.A.4DD.html\n",
      "\n",
      "[爆卦] 地震\n",
      " /bbs/Gossiping/M.1592079651.A.04B.html\n",
      "\n",
      "震\n",
      " /bbs/Gossiping/M.1592079650.A.DC3.html\n",
      "\n",
      "台中地震\n",
      " /bbs/Gossiping/M.1592079647.A.799.html\n",
      "\n",
      "[爆卦] 地震\n",
      " /bbs/Gossiping/M.1592079643.A.464.html\n"
     ]
    }
   ],
   "source": [
    "for title in titles:\n",
    "    print(title.text, title.a[\"href\"])\n",
    "    link = \"https://www.ptt.cc/\" + title.a[\"href\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1592079643'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_id = title.a[\"href\"].split(sep = \"/\")[-1].split(sep = \".\")[1]\n",
    "article_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py:986: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.ptt.cc'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py:986: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.ptt.cc'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py:986: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.ptt.cc'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py:986: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.ptt.cc'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    }
   ],
   "source": [
    "# 進入搜尋頁面的第一篇文章\n",
    "link = \"https://www.ptt.cc/\" + title.a[\"href\"]\n",
    "\n",
    "# 18禁\n",
    "payload = {\n",
    "    \"from\" : \"/bbs/Gossiping/index.html\",\n",
    "    \"yes\"  : \"yes\"\n",
    "}\n",
    "rs = requests.session()\n",
    "res_subpage = rs.post(\"https://www.ptt.cc/ask/over18\", verify = False, data = payload)\n",
    "\n",
    "res_subpage = rs.get(link, verify = False)\n",
    "soup_subpage = BeautifulSoup(res_subpage.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[爆卦] 地震 - 看板 Gossiping - 批踢踢實業坊\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "批踢踢實業坊\n",
      "›\n",
      "看板 Gossiping\n",
      "關於我們\n",
      "聯絡資訊\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "返回看板\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "作者kps1247 (黑衣劍士)看板Gossiping標題[爆卦] 地震時間Sun Jun 14 04:20:13 2020\n",
      "\n",
      "搖搖搖搖\n",
      "趁亂告白囉\n",
      "幹剛剛還以為是因為喝醉的說\n",
      "※註:有電視或媒體有報導者，請勿使用爆卦!\n",
      "無重大八卦請勿使用此分類，否則視同濫用爆卦鬧板(文章退回、水桶6個月)\n",
      "※政治類爆卦改為政治分類標題\n",
      "\n",
      "未滿30繁體中文字 水桶3個月\n",
      "\n",
      "-----\n",
      "Sent from JPTT on my Sony G8232.\n",
      "\n",
      "--\n",
      "推 scuderiaf430:1F奈米屌01/06 18:31\n",
      "→ scuderiaf430:...01/06 18:31\n",
      "推 rookiebear:恭喜一樓 01/06 18:31\n",
      "→ scuderiaf430:更正4F01/06 18:31\n",
      "推 w113353:1F何苦呢? 5F也奈米屌 01/06 18:31\n",
      "\n",
      "--\n",
      "※ 發信站: 批踢踢實業坊(ptt.cc), 來自: 49.216.51.34 (臺灣)\n",
      "※ 文章網址: https://www.ptt.cc/bbs/Gossiping/M.1592079643.A.464.html\n",
      "※ 編輯: kps1247 (49.216.51.34 臺灣), 06/14/2020 04:36:14\n",
      "\n",
      "\n",
      "本網站已依台灣網站內容分級規定處理。此區域為限制級，未滿十八歲者不得瀏覽。\n",
      "\n",
      "\n",
      "  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n",
      "  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n",
      "  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n",
      "  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n",
      "\n",
      "  ga('create', 'UA-32365737-1', {\n",
      "    cookieDomain: 'ptt.cc',\n",
      "    legacyCookieDomain: 'ptt.cc'\n",
      "  });\n",
      "  ga('send', 'pageview');\n",
      "\n",
      "\n",
      "\n",
      "(function(){window['__CF$cv$params']={r:'5a300c58daaddacc',m:'fe88bb6b05c182be98fdebed261ec9ba604ee00c-1592095732-1800-AQDdQUT+UNslIoz8Khki1CqW7vms2cRnweYNnculn5q4GYateYvCDRJsDg40AA+LCiRifiB3e8iTvs3lTwkRWfVg/inG5sZm2NFcaGIraLmy6q88XiolvksEVnEbxKllbDeJ6OHwHBezkcpg/9KEKPqY0PAyhFZB9qCScry8BRCZ',s:[0x5fe8a0fced,0xeca24b2faa],}})();\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup_subpage.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<span class=\"article-meta-value\">kps1247 (黑衣劍士)</span>, <span class=\"article-meta-value\">Gossiping</span>, <span class=\"article-meta-value\">[爆卦] 地震</span>, <span class=\"article-meta-value\">Sun Jun 14 04:20:13 2020</span>]\n"
     ]
    }
   ],
   "source": [
    "#一篇文章的 作者、標題、時間、看板\n",
    "header = soup_subpage.find_all(\"span\", \"article-meta-value\")\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kps1247 (黑衣劍士)\n",
      "Gossiping\n",
      "[爆卦] 地震\n",
      "Sun Jun 14 04:20:13 2020\n"
     ]
    }
   ],
   "source": [
    "print(header[0].text) #author\n",
    "print(header[1].text) #board\n",
    "print(header[2].text) #title\n",
    "print(header[3].text) #date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "作者kps1247 (黑衣劍士)看板Gossiping標題[爆卦] 地震時間Sun Jun 14 04:20:13 2020\n",
      "\n",
      "搖搖搖搖\n",
      "趁亂告白囉\n",
      "幹剛剛還以為是因為喝醉的說\n",
      "※註:有電視或媒體有報導者，請勿使用爆卦!\n",
      "無重大八卦請勿使用此分類，否則視同濫用爆卦鬧板(文章退回、水桶6個月)\n",
      "※政治類爆卦改為政治分類標題\n",
      "\n",
      "未滿30繁體中文字 水桶3個月\n",
      "\n",
      "-----\n",
      "Sent from JPTT on my Sony G8232.\n",
      "\n",
      "--\n",
      "推 scuderiaf430:1F奈米屌01/06 18:31\n",
      "→ scuderiaf430:...01/06 18:31\n",
      "推 rookiebear:恭喜一樓 01/06 18:31\n",
      "→ scuderiaf430:更正4F01/06 18:31\n",
      "推 w113353:1F何苦呢? 5F也奈米屌 01/06 18:31\n",
      "\n",
      "--\n",
      "※ 發信站: 批踢踢實業坊(ptt.cc), 來自: 49.216.51.34 (臺灣)\n",
      "※ 文章網址: https://www.ptt.cc/bbs/Gossiping/M.1592079643.A.464.html\n",
      "※ 編輯: kps1247 (49.216.51.34 臺灣), 06/14/2020 04:36:14\n",
      "\n",
      "\n",
      "本網站已依台灣網站內容分級規定處理。此區域為限制級，未滿十八歲者不得瀏覽。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_text = soup_subpage.find(id = \"main-container\").text\n",
    "print(main_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['※ 編輯: kps1247 (49.216.51.34 臺灣), 06/14/2020 04:36:14',\n",
       " '',\n",
       " '',\n",
       " '本網站已依台灣網站內容分級規定處理。此區域為限制級，未滿十八歲者不得瀏覽。',\n",
       " '']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#推文\n",
    "main_text.split(sep = \"html\")[1].split(sep = \"\\n\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "內容：\n",
      "搖搖搖搖\n",
      "趁亂告白囉\n",
      "幹剛剛還以為是因為喝醉的說\n",
      "※註:有電視或媒體有報導者，請勿使用爆卦!\n",
      "無重大八卦請勿使用此分類，否則視同濫用爆卦鬧板(文章退回、水桶6個月)\n",
      "※政治類爆卦改為政治分類標題\n",
      "\n",
      "未滿30繁體中文字 水桶3個月\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 把整個內容切割透過 \"-- \" 切割成2個陣列\n",
    "pre_text = main_text.split('--')[0]\n",
    "    \n",
    "# 把每段文字 根據 '\\n' 切開\n",
    "texts = pre_text.split('\\n')\n",
    "# 如果你爬多篇你會發現 \n",
    "contents = texts[2:]\n",
    "# 內容\n",
    "content = '\\n'.join(contents)\n",
    "\n",
    "print('內容：'+content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 49.216.51.34 (臺灣)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IP_address = main_text.split('來自:')[1].split(\"\\n\")[0]\n",
    "IP_address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "f\"{article_id}\" = {\n",
    "    \"Date\" : \"\"\n",
    "    \"Title\" : \"\"\n",
    "    \"Author\" : \"\"\n",
    "    \"IP\" : \"\"\n",
    "    \"Content\" : \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1592079643'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{article_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M1592079643'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec_str = \"M\" + f\"{article_id}\"\n",
    "exec_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(exec_str+\" = {}\", globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'M1590366890' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-16eb6f30d8fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM1590366890\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'M1590366890' is not defined"
     ]
    }
   ],
   "source": [
    "type(M1590366890)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'M1590366890' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-33c6da610692>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mM1590366890\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"01\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mM1590366890\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"02\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'M1590366890' is not defined"
     ]
    }
   ],
   "source": [
    "M1590366890[1] = \"01\"\n",
    "M1590366890[2] = \"02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(f\"{exec_str}\", globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 實作：把搜尋頁第一頁的文章都爬下來，存在一個dict內"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.ptt.cc/bbs/Gossiping/search?q=%E5%9C%B0%E9%9C%87 //查詢地震時的endpoint\n",
    "#search_endpoint_url = 'https://www.ptt.cc/bbs/Gossiping/search'\n",
    "# 把要查詢的關鍵字參數化\n",
    "#resp = requests.get(search_endpoint_url, params={'q': '地震'}, headers = header_over18)\n",
    "\n",
    "#soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "#忽略warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 建立字典\n",
    "earthquake_dict = {} # key是文章的link，value是另一本以文章的unique link為名的dict\n",
    "# list內則有：發文時間、文章標題、作者ID、作者IP、文章內容\n",
    "names = globals()\n",
    "\n",
    "# 搜尋頁面換頁\n",
    "# https://www.ptt.cc/bbs/Gossiping/search?page= 3 &q=%E5%9C%B0%E9%9C%87\n",
    "# key換成index\n",
    "index = 1\n",
    "for page_count in range(1, 52+1):\n",
    "    #search_page_url = \"https://www.ptt.cc/bbs/Gossiping/search?page=\" + str(page_count) + \"&q=%E5%9C%B0%E9%9C%87\"\n",
    "    search_page_url = \"https://www.ptt.cc/bbs/Gossiping/search?page=\" + str(page_count) + \"&q=震\"\n",
    "    resp = requests.get(search_page_url, headers = header_over18)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    \n",
    "    # 找到該搜尋頁面下 全部地震文章的連結\n",
    "    titles = soup.find_all(\"div\", \"title\") #找到每篇文章的標題及連結\n",
    "    for title in titles:\n",
    "        #print(title.text, title.a[\"href\"])\n",
    "        \n",
    "        link = \"https://www.ptt.cc/\" + title.a[\"href\"]\n",
    "        article_id = title.a[\"href\"].split(sep = \"/\")[-1].split(sep = \".\")[1]\n",
    "        article_id = index\n",
    "        index+=1\n",
    "        \n",
    "        #為這篇文章建一個dictionary\n",
    "        name_str = \"M\" + f\"{article_id}\"\n",
    "        names[\"M_%s\" %article_id] = {}\n",
    "        earthquake_dict[name_str] = names[\"M_%s\" %article_id]\n",
    "\n",
    "        \n",
    "        # 進入搜尋頁面的第一篇文章\n",
    "        payload = {\n",
    "            \"from\" : \"/bbs/Gossiping/index.html\",\n",
    "            \"yes\"  : \"yes\"\n",
    "        }\n",
    "        rs = requests.session()\n",
    "        res_subpage = rs.post(\"https://www.ptt.cc/ask/over18\", verify = False, data = payload)\n",
    "        res_subpage = rs.get(link, verify = False)\n",
    "        soup_subpage = BeautifulSoup(res_subpage.text, \"html.parser\")\n",
    "\n",
    "        #一篇文章的 作者、標題、時間、看板\n",
    "        header = soup_subpage.find_all(\"span\", \"article-meta-value\")\n",
    "        #print(header)\n",
    "        #print(header[0].text) #author\n",
    "        #print(header[1].text) #board\n",
    "        #print(header[2].text) #title\n",
    "        #print(header[3].text) #date\n",
    "        \n",
    "        try: #如果有「看板」的話\n",
    "            names[\"M_%s\" %article_id][\"Author\"] = header[0].text\n",
    "            names[\"M_%s\" %article_id][\"Board\"] = header[1].text\n",
    "            names[\"M_%s\" %article_id][\"Title\"] = header[2].text\n",
    "            names[\"M_%s\" %article_id][\"Date\"] = header[3].text\n",
    "        except:\n",
    "            try: #如果是回覆文章，沒有「看板」的話\n",
    "                names[\"M_%s\" %article_id][\"Author\"] = header[0].text\n",
    "                names[\"M_%s\" %article_id][\"Title\"] = header[1].text\n",
    "                names[\"M_%s\" %article_id][\"Date\"] = header[2].text\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # 內文\n",
    "        try:\n",
    "            main_text = soup_subpage.find(id = \"main-container\").text\n",
    "        except:\n",
    "            pass\n",
    "        #print(main_text)\n",
    "        # 把整個內容切割透過 \"-- \" 切割成2個陣列\n",
    "        pre_text = main_text.split('--')[0]\n",
    "        # 把每段文字 根據 '\\n' 切開\n",
    "        texts = pre_text.split('\\n')\n",
    "        # 如果你爬多篇你會發現 \n",
    "        contents = texts[2:]\n",
    "        # 內容\n",
    "        content = '\\n'.join(contents)\n",
    "        #print('內容：'+content)\n",
    "        names[\"M_%s\" %article_id][\"Content\"] = content\n",
    "\n",
    "        \n",
    "        #IP位址\n",
    "        try:\n",
    "            try:\n",
    "                IP_address = main_text.split('來自:')[1].split(\"\\n\")[0]\n",
    "                #print(IP_address)\n",
    "                names[\"M_%s\" %article_id][\"Full IP\"] = IP_address\n",
    "            except:\n",
    "                try:\n",
    "                    author = header[0].text\n",
    "                    authorID = author.split(sep = \" (\")[0]\n",
    "                    #print(main_text.split(sep = authorID)[2].split(sep = \"),\")[0].split()[0][1:])\n",
    "                    IP_address = main_text.split(sep = authorID)[2].split(sep = \"),\")[0].split()[0][1:]\n",
    "                    names[\"M_%s\" %article_id][\"Full IP\"] = IP_address\n",
    "                except:\n",
    "                    pass\n",
    "        except:\n",
    "            names[\"M_%s\" %article_id][\"Full IP\"] = \"Not Found\"\n",
    "        \n",
    "        #只有數字的IP\n",
    "        try:\n",
    "            IP_shorten = IP_address.split(sep = \" \")[1]\n",
    "            names[\"M_%s\" %article_id][\"IP\"] = IP_shorten\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # 推文\n",
    "        try:            \n",
    "            push = main_text.split(sep = \"html\")[1].split(sep = \"\\n\")[1:]\n",
    "            names[\"M_%s\" %article_id][\"Push\"] = push\n",
    "        except:\n",
    "            names[\"M_%s\" %article_id][\"Push\"] = \"No Push\"\n",
    "        \n",
    "        time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 八卦版前一百頁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 八卦版前一百頁\n",
    "# https://www.ptt.cc/bbs/Gossiping/search?q=%E5%9C%B0%E9%9C%87 //查詢地震時的endpoint\n",
    "#search_endpoint_url = 'https://www.ptt.cc/bbs/Gossiping/search'\n",
    "# 把要查詢的關鍵字參數化\n",
    "#resp = requests.get(search_endpoint_url, params={'q': '地震'}, headers = header_over18)\n",
    "\n",
    "#soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "#忽略warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 建立字典\n",
    "earthquake_dict = {} # key是文章的link，value是另一本以文章的unique link為名的dict\n",
    "# list內則有：發文時間、文章標題、作者ID、作者IP、文章內容\n",
    "names = globals()\n",
    "\n",
    "# 搜尋頁面換頁\n",
    "# https://www.ptt.cc/bbs/Gossiping/search?page= 3 &q=%E5%9C%B0%E9%9C%87\n",
    "# key換成index\n",
    "index = 1\n",
    "for page_count in range(39284, 39184, -1):\n",
    "    #search_page_url = \"https://www.ptt.cc/bbs/Gossiping/search?page=\" + str(page_count) + \"&q=%E5%9C%B0%E9%9C%87\"\n",
    "    #search_page_url = \"https://www.ptt.cc/bbs/Gossiping/search?page=\" + str(page_count) + \"&q=震\"\n",
    "    search_page_url = \"https://www.ptt.cc/bbs/Gossiping/index\" + str(page_count) + \".html\"\n",
    "    resp = requests.get(search_page_url, headers = header_over18)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    \n",
    "    # 找到該搜尋頁面下 全部地震文章的連結\n",
    "    titles = soup.find_all(\"div\", \"title\") #找到每篇文章的標題及連結\n",
    "    for title in titles:\n",
    "        #print(title.text, title.a[\"href\"])\n",
    "        \n",
    "        try:\n",
    "            link = \"https://www.ptt.cc/\" + title.a[\"href\"]\n",
    "        except:\n",
    "            pass\n",
    "        #article_id = title.a[\"href\"].split(sep = \"/\")[-1].split(sep = \".\")[1]\n",
    "        article_id = index\n",
    "        index+=1\n",
    "        \n",
    "        #為這篇文章建一個dictionary\n",
    "        name_str = \"M\" + f\"{article_id}\"\n",
    "        names[\"M_%s\" %article_id] = {}\n",
    "        earthquake_dict[name_str] = names[\"M_%s\" %article_id]\n",
    "\n",
    "        \n",
    "        # 進入搜尋頁面的第一篇文章\n",
    "        payload = {\n",
    "            \"from\" : \"/bbs/Gossiping/index.html\",\n",
    "            \"yes\"  : \"yes\"\n",
    "        }\n",
    "        rs = requests.session()\n",
    "        res_subpage = rs.post(\"https://www.ptt.cc/ask/over18\", verify = False, data = payload)\n",
    "        res_subpage = rs.get(link, verify = False)\n",
    "        soup_subpage = BeautifulSoup(res_subpage.text, \"html.parser\")\n",
    "\n",
    "        #一篇文章的 作者、標題、時間、看板\n",
    "        header = soup_subpage.find_all(\"span\", \"article-meta-value\")\n",
    "        #print(header)\n",
    "        #print(header[0].text) #author\n",
    "        #print(header[1].text) #board\n",
    "        #print(header[2].text) #title\n",
    "        #print(header[3].text) #date\n",
    "        \n",
    "        try: #如果有「看板」的話\n",
    "            names[\"M_%s\" %article_id][\"Author\"] = header[0].text\n",
    "            names[\"M_%s\" %article_id][\"Board\"] = header[1].text\n",
    "            names[\"M_%s\" %article_id][\"Title\"] = header[2].text\n",
    "            names[\"M_%s\" %article_id][\"Date\"] = header[3].text\n",
    "        except:\n",
    "            try: #如果是回覆文章，沒有「看板」的話\n",
    "                names[\"M_%s\" %article_id][\"Author\"] = header[0].text\n",
    "                names[\"M_%s\" %article_id][\"Title\"] = header[1].text\n",
    "                names[\"M_%s\" %article_id][\"Date\"] = header[2].text\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # 內文\n",
    "        try:\n",
    "            main_text = soup_subpage.find(id = \"main-container\").text\n",
    "        except:\n",
    "            pass\n",
    "        #print(main_text)\n",
    "        # 把整個內容切割透過 \"-- \" 切割成2個陣列\n",
    "        pre_text = main_text.split('--')[0]\n",
    "        # 把每段文字 根據 '\\n' 切開\n",
    "        texts = pre_text.split('\\n')\n",
    "        # 如果你爬多篇你會發現 \n",
    "        contents = texts[2:]\n",
    "        # 內容\n",
    "        content = '\\n'.join(contents)\n",
    "        #print('內容：'+content)\n",
    "        names[\"M_%s\" %article_id][\"Content\"] = content\n",
    "\n",
    "        \n",
    "        #IP位址\n",
    "        try:\n",
    "            try:\n",
    "                IP_address = main_text.split('來自:')[1].split(\"\\n\")[0]\n",
    "                #print(IP_address)\n",
    "                names[\"M_%s\" %article_id][\"Full IP\"] = IP_address\n",
    "            except:\n",
    "                try:\n",
    "                    author = header[0].text\n",
    "                    authorID = author.split(sep = \" (\")[0]\n",
    "                    #print(main_text.split(sep = authorID)[2].split(sep = \"),\")[0].split()[0][1:])\n",
    "                    IP_address = main_text.split(sep = authorID)[2].split(sep = \"),\")[0].split()[0][1:]\n",
    "                    names[\"M_%s\" %article_id][\"Full IP\"] = IP_address\n",
    "                except:\n",
    "                    pass\n",
    "        except:\n",
    "            names[\"M_%s\" %article_id][\"Full IP\"] = \"Not Found\"\n",
    "        \n",
    "        #只有數字的IP\n",
    "        try:\n",
    "            IP_shorten = IP_address.split(sep = \" \")[1]\n",
    "            names[\"M_%s\" %article_id][\"IP\"] = IP_shorten\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # 推文\n",
    "        try:            \n",
    "            push = main_text.split(sep = \"html\")[1].split(sep = \"\\n\")[1:]\n",
    "            names[\"M_%s\" %article_id][\"Push\"] = push\n",
    "        except:\n",
    "            names[\"M_%s\" %article_id][\"Push\"] = \"No Push\"\n",
    "        \n",
    "        time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author = header[0].text\n",
    "authorID = author.split(sep = \" (\")[0]\n",
    "#print(main_text.split(sep = authorID)[2].split(sep = \"),\")[0].split()[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(earthquake_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_dict[\"M3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IP_shorten = IP_address.split(sep = \" \")[1]\n",
    "IP_shorten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "np.save(\"earthquake_dict_0614中型有感地震\", earthquake_dict)\n",
    "\n",
    "# Load\n",
    "#read_dictionary = np.load('my_file.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_dict[\"M20\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
