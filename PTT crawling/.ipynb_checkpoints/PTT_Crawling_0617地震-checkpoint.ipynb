{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬PTT地震文\n",
    "\n",
    "* 需要的資訊：\n",
    "    \n",
    "    1. 發文時間\n",
    "    2. 文章標題\n",
    "    3. 文章內容\n",
    "    4. 前100推推文內容\n",
    "    5. 發文者ID\n",
    "    6. 發文者IP位址\n",
    "    7. 該文是否屬於地震文（dummy）\n",
    "    8. 該文是否搶到地震爆文（dummy）\n",
    "   \n",
    "* 另外要寫的功能：\n",
    "\n",
    "    1. 用IP位址反查地址資訊，回傳「縣市」（str）及經緯度（某種可以被餵進google map API的格式）\n",
    "    2. 一天／一週當中PTT上站人數的pattern，輔以「當時發文者發文所處的時間」來控制error term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.25.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# 導入 模組(module) \n",
    "import requests \n",
    "# 導入 BeautifulSoup 模組(module)：解析HTML 語法工具\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from multiprocessing import Pool #https://github.com/leVirve/CrawlerTutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#八卦版文章連結\n",
    "url = \"https://www.ptt.cc/bbs/Gossiping/index.html\"\n",
    "header_over18 = {\"cookie\":\"over18=1\"}\n",
    "res = requests.get(url, headers = header_over18)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try to search keyword \"地震\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.ptt.cc/bbs/Gossiping/search?q=%E5%9C%B0%E9%9C%87 //查詢地震時的endpoint\n",
    "search_endpoint_url = 'https://www.ptt.cc/bbs/Gossiping/search'\n",
    "# 把要查詢的關鍵字參數化\n",
    "resp = requests.get(search_endpoint_url, params={'q': '震'}, headers = header_over18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(resp.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找到全部地震文章的連結\n",
    "links = soup.find_all(\"a href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = soup.find_all(\"div\", \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[問卦] 地震搖晃程度和屋齡有關嗎？\n",
      " /bbs/Gossiping/M.1592355119.A.397.html\n",
      "\n",
      "[爆卦] 地震\n",
      " /bbs/Gossiping/M.1592349530.A.F00.html\n",
      "\n",
      "[爆卦] 地震\n",
      " /bbs/Gossiping/M.1592349499.A.FF3.html\n",
      "\n",
      "[爆卦] 地震\n",
      " /bbs/Gossiping/M.1592349494.A.959.html\n",
      "\n",
      "地震\n",
      " /bbs/Gossiping/M.1592349492.A.549.html\n",
      "\n",
      "[爆卦] 地震\n",
      " /bbs/Gossiping/M.1592349487.A.636.html\n",
      "\n",
      "地震\n",
      " /bbs/Gossiping/M.1592349482.A.D1B.html\n",
      "\n",
      "[爆卦] 地震!\n",
      " /bbs/Gossiping/M.1592349476.A.46C.html\n",
      "\n",
      "[爆卦] 地震\n",
      " /bbs/Gossiping/M.1592349474.A.EC6.html\n",
      "\n",
      "地震\n",
      " /bbs/Gossiping/M.1592349473.A.B7A.html\n",
      "\n",
      "[爆卦] 地震\n",
      " /bbs/Gossiping/M.1592349459.A.902.html\n",
      "\n",
      "Re: [問卦] 功高震主時，要怎麼樣給主管台階下\n",
      " /bbs/Gossiping/M.1592330099.A.335.html\n",
      "\n",
      "[問卦] 功高震主時，要怎麼樣給主管台階下\n",
      " /bbs/Gossiping/M.1592329647.A.976.html\n",
      "\n",
      "[新聞] 陳時中功高震主？被蔡英文、蘇貞昌圍剿？\n",
      " /bbs/Gossiping/M.1592295120.A.0EE.html\n",
      "\n",
      "[問卦] 華人演包青天把臉塗黑 黑人會震怒嗎\n",
      " /bbs/Gossiping/M.1592229107.A.383.html\n",
      "\n",
      "[問卦] 日本友人震驚：台灣竟是炒房天堂？\n",
      " /bbs/Gossiping/M.1592197082.A.D91.html\n",
      "\n",
      "地震\n",
      " /bbs/Gossiping/M.1592158410.A.D2E.html\n",
      "\n",
      "[爆卦] 地震\n",
      " /bbs/Gossiping/M.1592158328.A.3AC.html\n",
      "\n",
      "[爆卦]地震 阿芳我愛妳\n",
      " /bbs/Gossiping/M.1592158280.A.80C.html\n",
      "\n",
      "[問卦] 今天早上的地震很大嗎?\n",
      " /bbs/Gossiping/M.1592146308.A.988.html\n"
     ]
    }
   ],
   "source": [
    "for title in titles:\n",
    "    print(title.text, title.a[\"href\"])\n",
    "    link = \"https://www.ptt.cc/\" + title.a[\"href\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1592146308'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_id = title.a[\"href\"].split(sep = \"/\")[-1].split(sep = \".\")[1]\n",
    "article_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py:986: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.ptt.cc'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py:986: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.ptt.cc'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py:986: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.ptt.cc'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py:986: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.ptt.cc'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    }
   ],
   "source": [
    "# 進入搜尋頁面的第一篇文章\n",
    "link = \"https://www.ptt.cc/\" + title.a[\"href\"]\n",
    "\n",
    "# 18禁\n",
    "payload = {\n",
    "    \"from\" : \"/bbs/Gossiping/index.html\",\n",
    "    \"yes\"  : \"yes\"\n",
    "}\n",
    "rs = requests.session()\n",
    "res_subpage = rs.post(\"https://www.ptt.cc/ask/over18\", verify = False, data = payload)\n",
    "\n",
    "res_subpage = rs.get(link, verify = False)\n",
    "soup_subpage = BeautifulSoup(res_subpage.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[問卦] 今天早上的地震很大嗎? - 看板 Gossiping - 批踢踢實業坊\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "批踢踢實業坊\n",
      "›\n",
      "看板 Gossiping\n",
      "關於我們\n",
      "聯絡資訊\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "返回看板\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "作者wurara22 (烏拉餓餓)看板Gossiping標題[問卦] 今天早上的地震很大嗎?時間Sun Jun 14 22:51:41 2020\n",
      "大家晚安\n",
      "如題\n",
      "\n",
      "剛剛看新聞才看到今天早上4:19有地震\n",
      "還說是全台有感 規模6.0\n",
      "\n",
      "可是魯妹根本沒感覺...\n",
      "\n",
      "之前半夜有地震的話我都會醒來\n",
      "\n",
      "\n",
      "昨晚跟平常不同的是有戴耳塞睡覺\n",
      "可能因為耳塞沒聽到家具震動的聲音所以沒注意到地震??\n",
      "\n",
      "有沒有八卦?0'_'0\n",
      "\n",
      "--\n",
      "——————\n",
      "咚踏取蜜！\n",
      "https://i.imgur.com/ymESrcv.jpg\n",
      "(●’ω`●）ノ dustycat_udon\n",
      "——————\n",
      "\n",
      "--\n",
      "※ 發信站: 批踢踢實業坊(ptt.cc), 來自: 118.166.126.3 (臺灣)\n",
      "※ 文章網址: https://www.ptt.cc/bbs/Gossiping/M.1592146308.A.988.html\n",
      "→ akila08539: 灰塵貓有感覺嗎 06/14 22:52\n",
      "今天早上的因為我沒感覺所以也不知道灰塵貓怎樣\n",
      "不過之前有地震的時候她是沒什麼反應，頂多抬頭四處觀望\n",
      "學姐貓是完全睡死zzz\n",
      "\n",
      "我是有在床頭櫃放一個洗衣袋啦，有需要把她包進去一起逃\n",
      "噓 jenn8588: 沒人關心 06/14 22:52\n",
      "推 XXXXLINDA: 還好 06/14 22:52\n",
      "→ saobox: 意淫地震 8888888 06/14 22:52\n",
      "推 purplebfly: 事實上我是被手機吵醒,不是被地震吵醒的 06/14 22:52\n",
      "這也很奇怪\n",
      "之前我會收到警報，但是今天早上沒有\n",
      "推 wade6510: 最大三級  因為在海裡 算中層深度 06/14 22:52\n",
      "※ 編輯: wurara22 (118.166.126.3 臺灣), 06/14/2020 22:56:08\n",
      "推 Xhocer: 我睡得跟死豬依樣 06/14 22:52\n",
      "推 dWoWb: 是喔...早上起來看八卦版也沒特別看到地震 LUL 06/14 22:54\n",
      "因為新聞寫是今年最強，但是我卻睡到翻，所以想知道到底是不是今年最強\n",
      "※ 編輯: wurara22 (118.166.126.3 臺灣), 06/14/2020 22:57:05\n",
      "→ SFGiants: 美國無感 XD 06/14 23:01\n",
      "推 ted01234567: 有感 但是太累了繼續睡 06/14 23:43\n",
      "→ a3221715: 睡不好 不知道是不是因為地震 06/14 23:55\n",
      "→ a3221715: 但睡死無感 06/14 23:55\n",
      "推 cuteSquirrel: 很晃 06/15 00:05\n",
      "推 tuutu: 有感 我的鸚鵡被驚醒亂飛 後來我把他抓來一起睡 06/15 01:53\n",
      "\n",
      "\n",
      "本網站已依台灣網站內容分級規定處理。此區域為限制級，未滿十八歲者不得瀏覽。\n",
      "\n",
      "\n",
      "  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n",
      "  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n",
      "  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n",
      "  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n",
      "\n",
      "  ga('create', 'UA-32365737-1', {\n",
      "    cookieDomain: 'ptt.cc',\n",
      "    legacyCookieDomain: 'ptt.cc'\n",
      "  });\n",
      "  ga('send', 'pageview');\n",
      "\n",
      "\n",
      "\n",
      "(function(){window['__CF$cv$params']={r:'5a48f3e9a87ef085',m:'b5cbb8555d5fb0e4f5bb8909ea79324955a039d6-1592356875-1800-AUQIbWIzpZqF/Xn9zjAWZeLr4yHzQfrhtqu9SU0fIiOJAYSafo5SJwJDCnwVvD2QAEhIdLYof43IAz1cTXj2C2A+FWumT8Notohx4OYqJw6kRstEAl0DlYRrWdtrpCSJInIssOGnGblOHeffwKlTRySfAQ2mJbLagcKPbwqNzZHe',s:[0x831762bf8f,0x9ec9cbe4da],}})();\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup_subpage.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<span class=\"article-meta-value\">wurara22 (烏拉餓餓)</span>, <span class=\"article-meta-value\">Gossiping</span>, <span class=\"article-meta-value\">[問卦] 今天早上的地震很大嗎?</span>, <span class=\"article-meta-value\">Sun Jun 14 22:51:41 2020</span>]\n"
     ]
    }
   ],
   "source": [
    "#一篇文章的 作者、標題、時間、看板\n",
    "header = soup_subpage.find_all(\"span\", \"article-meta-value\")\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wurara22 (烏拉餓餓)\n",
      "Gossiping\n",
      "[問卦] 今天早上的地震很大嗎?\n",
      "Sun Jun 14 22:51:41 2020\n"
     ]
    }
   ],
   "source": [
    "print(header[0].text) #author\n",
    "print(header[1].text) #board\n",
    "print(header[2].text) #title\n",
    "print(header[3].text) #date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "作者wurara22 (烏拉餓餓)看板Gossiping標題[問卦] 今天早上的地震很大嗎?時間Sun Jun 14 22:51:41 2020\n",
      "大家晚安\n",
      "如題\n",
      "\n",
      "剛剛看新聞才看到今天早上4:19有地震\n",
      "還說是全台有感 規模6.0\n",
      "\n",
      "可是魯妹根本沒感覺...\n",
      "\n",
      "之前半夜有地震的話我都會醒來\n",
      "\n",
      "\n",
      "昨晚跟平常不同的是有戴耳塞睡覺\n",
      "可能因為耳塞沒聽到家具震動的聲音所以沒注意到地震??\n",
      "\n",
      "有沒有八卦?0'_'0\n",
      "\n",
      "--\n",
      "——————\n",
      "咚踏取蜜！\n",
      "https://i.imgur.com/ymESrcv.jpg\n",
      "(●’ω`●）ノ dustycat_udon\n",
      "——————\n",
      "\n",
      "--\n",
      "※ 發信站: 批踢踢實業坊(ptt.cc), 來自: 118.166.126.3 (臺灣)\n",
      "※ 文章網址: https://www.ptt.cc/bbs/Gossiping/M.1592146308.A.988.html\n",
      "→ akila08539: 灰塵貓有感覺嗎 06/14 22:52\n",
      "今天早上的因為我沒感覺所以也不知道灰塵貓怎樣\n",
      "不過之前有地震的時候她是沒什麼反應，頂多抬頭四處觀望\n",
      "學姐貓是完全睡死zzz\n",
      "\n",
      "我是有在床頭櫃放一個洗衣袋啦，有需要把她包進去一起逃\n",
      "噓 jenn8588: 沒人關心 06/14 22:52\n",
      "推 XXXXLINDA: 還好 06/14 22:52\n",
      "→ saobox: 意淫地震 8888888 06/14 22:52\n",
      "推 purplebfly: 事實上我是被手機吵醒,不是被地震吵醒的 06/14 22:52\n",
      "這也很奇怪\n",
      "之前我會收到警報，但是今天早上沒有\n",
      "推 wade6510: 最大三級  因為在海裡 算中層深度 06/14 22:52\n",
      "※ 編輯: wurara22 (118.166.126.3 臺灣), 06/14/2020 22:56:08\n",
      "推 Xhocer: 我睡得跟死豬依樣 06/14 22:52\n",
      "推 dWoWb: 是喔...早上起來看八卦版也沒特別看到地震 LUL 06/14 22:54\n",
      "因為新聞寫是今年最強，但是我卻睡到翻，所以想知道到底是不是今年最強\n",
      "※ 編輯: wurara22 (118.166.126.3 臺灣), 06/14/2020 22:57:05\n",
      "→ SFGiants: 美國無感 XD 06/14 23:01\n",
      "推 ted01234567: 有感 但是太累了繼續睡 06/14 23:43\n",
      "→ a3221715: 睡不好 不知道是不是因為地震 06/14 23:55\n",
      "→ a3221715: 但睡死無感 06/14 23:55\n",
      "推 cuteSquirrel: 很晃 06/15 00:05\n",
      "推 tuutu: 有感 我的鸚鵡被驚醒亂飛 後來我把他抓來一起睡 06/15 01:53\n",
      "\n",
      "\n",
      "本網站已依台灣網站內容分級規定處理。此區域為限制級，未滿十八歲者不得瀏覽。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_text = soup_subpage.find(id = \"main-container\").text\n",
    "print(main_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['→ akila08539: 灰塵貓有感覺嗎 06/14 22:52',\n",
       " '今天早上的因為我沒感覺所以也不知道灰塵貓怎樣',\n",
       " '不過之前有地震的時候她是沒什麼反應，頂多抬頭四處觀望',\n",
       " '學姐貓是完全睡死zzz',\n",
       " '',\n",
       " '我是有在床頭櫃放一個洗衣袋啦，有需要把她包進去一起逃',\n",
       " '噓 jenn8588: 沒人關心 06/14 22:52',\n",
       " '推 XXXXLINDA: 還好 06/14 22:52',\n",
       " '→ saobox: 意淫地震 8888888 06/14 22:52',\n",
       " '推 purplebfly: 事實上我是被手機吵醒,不是被地震吵醒的 06/14 22:52',\n",
       " '這也很奇怪',\n",
       " '之前我會收到警報，但是今天早上沒有',\n",
       " '推 wade6510: 最大三級  因為在海裡 算中層深度 06/14 22:52',\n",
       " '※ 編輯: wurara22 (118.166.126.3 臺灣), 06/14/2020 22:56:08',\n",
       " '推 Xhocer: 我睡得跟死豬依樣 06/14 22:52',\n",
       " '推 dWoWb: 是喔...早上起來看八卦版也沒特別看到地震 LUL 06/14 22:54',\n",
       " '因為新聞寫是今年最強，但是我卻睡到翻，所以想知道到底是不是今年最強',\n",
       " '※ 編輯: wurara22 (118.166.126.3 臺灣), 06/14/2020 22:57:05',\n",
       " '→ SFGiants: 美國無感 XD 06/14 23:01',\n",
       " '推 ted01234567: 有感 但是太累了繼續睡 06/14 23:43',\n",
       " '→ a3221715: 睡不好 不知道是不是因為地震 06/14 23:55',\n",
       " '→ a3221715: 但睡死無感 06/14 23:55',\n",
       " '推 cuteSquirrel: 很晃 06/15 00:05',\n",
       " '推 tuutu: 有感 我的鸚鵡被驚醒亂飛 後來我把他抓來一起睡 06/15 01:53',\n",
       " '',\n",
       " '',\n",
       " '本網站已依台灣網站內容分級規定處理。此區域為限制級，未滿十八歲者不得瀏覽。',\n",
       " '']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#推文\n",
    "main_text.split(sep = \"html\")[1].split(sep = \"\\n\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "內容：大家晚安\n",
      "如題\n",
      "\n",
      "剛剛看新聞才看到今天早上4:19有地震\n",
      "還說是全台有感 規模6.0\n",
      "\n",
      "可是魯妹根本沒感覺...\n",
      "\n",
      "之前半夜有地震的話我都會醒來\n",
      "\n",
      "\n",
      "昨晚跟平常不同的是有戴耳塞睡覺\n",
      "可能因為耳塞沒聽到家具震動的聲音所以沒注意到地震??\n",
      "\n",
      "有沒有八卦?0'_'0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 把整個內容切割透過 \"-- \" 切割成2個陣列\n",
    "pre_text = main_text.split('--')[0]\n",
    "    \n",
    "# 把每段文字 根據 '\\n' 切開\n",
    "texts = pre_text.split('\\n')\n",
    "# 如果你爬多篇你會發現 \n",
    "contents = texts[2:]\n",
    "# 內容\n",
    "content = '\\n'.join(contents)\n",
    "\n",
    "print('內容：'+content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 118.166.126.3 (臺灣)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IP_address = main_text.split('來自:')[1].split(\"\\n\")[0]\n",
    "IP_address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "f\"{article_id}\" = {\n",
    "    \"Date\" : \"\"\n",
    "    \"Title\" : \"\"\n",
    "    \"Author\" : \"\"\n",
    "    \"IP\" : \"\"\n",
    "    \"Content\" : \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1592146308'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{article_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M1592146308'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec_str = \"M\" + f\"{article_id}\"\n",
    "exec_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(exec_str+\" = {}\", globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'M1590366890' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-16eb6f30d8fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM1590366890\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'M1590366890' is not defined"
     ]
    }
   ],
   "source": [
    "type(M1590366890)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'M1590366890' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-33c6da610692>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mM1590366890\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"01\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mM1590366890\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"02\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'M1590366890' is not defined"
     ]
    }
   ],
   "source": [
    "M1590366890[1] = \"01\"\n",
    "M1590366890[2] = \"02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(f\"{exec_str}\", globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 實作：把搜尋頁第一頁的文章都爬下來，存在一個dict內"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.ptt.cc/bbs/Gossiping/search?q=%E5%9C%B0%E9%9C%87 //查詢地震時的endpoint\n",
    "#search_endpoint_url = 'https://www.ptt.cc/bbs/Gossiping/search'\n",
    "# 把要查詢的關鍵字參數化\n",
    "#resp = requests.get(search_endpoint_url, params={'q': '地震'}, headers = header_over18)\n",
    "\n",
    "#soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "#忽略warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 建立字典\n",
    "earthquake_dict = {} # key是文章的link，value是另一本以文章的unique link為名的dict\n",
    "# list內則有：發文時間、文章標題、作者ID、作者IP、文章內容\n",
    "names = globals()\n",
    "\n",
    "# 搜尋頁面換頁\n",
    "# https://www.ptt.cc/bbs/Gossiping/search?page= 3 &q=%E5%9C%B0%E9%9C%87\n",
    "# key換成index\n",
    "index = 1\n",
    "for page_count in range(1, 52+1):\n",
    "    #search_page_url = \"https://www.ptt.cc/bbs/Gossiping/search?page=\" + str(page_count) + \"&q=%E5%9C%B0%E9%9C%87\"\n",
    "    search_page_url = \"https://www.ptt.cc/bbs/Gossiping/search?page=\" + str(page_count) + \"&q=震\"\n",
    "    resp = requests.get(search_page_url, headers = header_over18)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    \n",
    "    # 找到該搜尋頁面下 全部地震文章的連結\n",
    "    titles = soup.find_all(\"div\", \"title\") #找到每篇文章的標題及連結\n",
    "    for title in titles:\n",
    "        #print(title.text, title.a[\"href\"])\n",
    "        \n",
    "        link = \"https://www.ptt.cc/\" + title.a[\"href\"]\n",
    "        article_id = title.a[\"href\"].split(sep = \"/\")[-1].split(sep = \".\")[1]\n",
    "        article_id = index\n",
    "        index+=1\n",
    "        \n",
    "        #為這篇文章建一個dictionary\n",
    "        name_str = \"M\" + f\"{article_id}\"\n",
    "        names[\"M_%s\" %article_id] = {}\n",
    "        earthquake_dict[name_str] = names[\"M_%s\" %article_id]\n",
    "\n",
    "        \n",
    "        # 進入搜尋頁面的第一篇文章\n",
    "        payload = {\n",
    "            \"from\" : \"/bbs/Gossiping/index.html\",\n",
    "            \"yes\"  : \"yes\"\n",
    "        }\n",
    "        rs = requests.session()\n",
    "        res_subpage = rs.post(\"https://www.ptt.cc/ask/over18\", verify = False, data = payload)\n",
    "        res_subpage = rs.get(link, verify = False)\n",
    "        soup_subpage = BeautifulSoup(res_subpage.text, \"html.parser\")\n",
    "\n",
    "        #一篇文章的 作者、標題、時間、看板\n",
    "        header = soup_subpage.find_all(\"span\", \"article-meta-value\")\n",
    "        #print(header)\n",
    "        #print(header[0].text) #author\n",
    "        #print(header[1].text) #board\n",
    "        #print(header[2].text) #title\n",
    "        #print(header[3].text) #date\n",
    "        \n",
    "        try: #如果有「看板」的話\n",
    "            names[\"M_%s\" %article_id][\"Author\"] = header[0].text\n",
    "            names[\"M_%s\" %article_id][\"Board\"] = header[1].text\n",
    "            names[\"M_%s\" %article_id][\"Title\"] = header[2].text\n",
    "            names[\"M_%s\" %article_id][\"Date\"] = header[3].text\n",
    "        except:\n",
    "            try: #如果是回覆文章，沒有「看板」的話\n",
    "                names[\"M_%s\" %article_id][\"Author\"] = header[0].text\n",
    "                names[\"M_%s\" %article_id][\"Title\"] = header[1].text\n",
    "                names[\"M_%s\" %article_id][\"Date\"] = header[2].text\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # 內文\n",
    "        try:\n",
    "            main_text = soup_subpage.find(id = \"main-container\").text\n",
    "        except:\n",
    "            pass\n",
    "        #print(main_text)\n",
    "        # 把整個內容切割透過 \"-- \" 切割成2個陣列\n",
    "        pre_text = main_text.split('--')[0]\n",
    "        # 把每段文字 根據 '\\n' 切開\n",
    "        texts = pre_text.split('\\n')\n",
    "        # 如果你爬多篇你會發現 \n",
    "        contents = texts[2:]\n",
    "        # 內容\n",
    "        content = '\\n'.join(contents)\n",
    "        #print('內容：'+content)\n",
    "        names[\"M_%s\" %article_id][\"Content\"] = content\n",
    "\n",
    "        \n",
    "        #IP位址\n",
    "        try:\n",
    "            try:\n",
    "                IP_address = main_text.split('來自:')[1].split(\"\\n\")[0]\n",
    "                #print(IP_address)\n",
    "                names[\"M_%s\" %article_id][\"Full IP\"] = IP_address\n",
    "            except:\n",
    "                try:\n",
    "                    author = header[0].text\n",
    "                    authorID = author.split(sep = \" (\")[0]\n",
    "                    #print(main_text.split(sep = authorID)[2].split(sep = \"),\")[0].split()[0][1:])\n",
    "                    IP_address = main_text.split(sep = authorID)[2].split(sep = \"),\")[0].split()[0][1:]\n",
    "                    names[\"M_%s\" %article_id][\"Full IP\"] = IP_address\n",
    "                except:\n",
    "                    pass\n",
    "        except:\n",
    "            names[\"M_%s\" %article_id][\"Full IP\"] = \"Not Found\"\n",
    "        \n",
    "        #只有數字的IP\n",
    "        try:\n",
    "            IP_shorten = IP_address.split(sep = \" \")[1]\n",
    "            names[\"M_%s\" %article_id][\"IP\"] = IP_shorten\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # 推文\n",
    "        try:            \n",
    "            push = main_text.split(sep = \"html\")[1].split(sep = \"\\n\")[1:]\n",
    "            names[\"M_%s\" %article_id][\"Push\"] = push\n",
    "        except:\n",
    "            names[\"M_%s\" %article_id][\"Push\"] = \"No Push\"\n",
    "        \n",
    "        time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 八卦版前一百頁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 八卦版前一百頁\n",
    "# https://www.ptt.cc/bbs/Gossiping/search?q=%E5%9C%B0%E9%9C%87 //查詢地震時的endpoint\n",
    "#search_endpoint_url = 'https://www.ptt.cc/bbs/Gossiping/search'\n",
    "# 把要查詢的關鍵字參數化\n",
    "#resp = requests.get(search_endpoint_url, params={'q': '地震'}, headers = header_over18)\n",
    "\n",
    "#soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "#忽略warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 建立字典\n",
    "earthquake_dict = {} # key是文章的link，value是另一本以文章的unique link為名的dict\n",
    "# list內則有：發文時間、文章標題、作者ID、作者IP、文章內容\n",
    "names = globals()\n",
    "\n",
    "# 搜尋頁面換頁\n",
    "# https://www.ptt.cc/bbs/Gossiping/search?page= 3 &q=%E5%9C%B0%E9%9C%87\n",
    "# key換成index\n",
    "index = 1\n",
    "for page_count in range(39284, 39184, -1):\n",
    "    #search_page_url = \"https://www.ptt.cc/bbs/Gossiping/search?page=\" + str(page_count) + \"&q=%E5%9C%B0%E9%9C%87\"\n",
    "    #search_page_url = \"https://www.ptt.cc/bbs/Gossiping/search?page=\" + str(page_count) + \"&q=震\"\n",
    "    search_page_url = \"https://www.ptt.cc/bbs/Gossiping/index\" + str(page_count) + \".html\"\n",
    "    resp = requests.get(search_page_url, headers = header_over18)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    \n",
    "    # 找到該搜尋頁面下 全部地震文章的連結\n",
    "    titles = soup.find_all(\"div\", \"title\") #找到每篇文章的標題及連結\n",
    "    for title in titles:\n",
    "        #print(title.text, title.a[\"href\"])\n",
    "        \n",
    "        try:\n",
    "            link = \"https://www.ptt.cc/\" + title.a[\"href\"]\n",
    "        except:\n",
    "            pass\n",
    "        #article_id = title.a[\"href\"].split(sep = \"/\")[-1].split(sep = \".\")[1]\n",
    "        article_id = index\n",
    "        index+=1\n",
    "        \n",
    "        #為這篇文章建一個dictionary\n",
    "        name_str = \"M\" + f\"{article_id}\"\n",
    "        names[\"M_%s\" %article_id] = {}\n",
    "        earthquake_dict[name_str] = names[\"M_%s\" %article_id]\n",
    "\n",
    "        \n",
    "        # 進入搜尋頁面的第一篇文章\n",
    "        payload = {\n",
    "            \"from\" : \"/bbs/Gossiping/index.html\",\n",
    "            \"yes\"  : \"yes\"\n",
    "        }\n",
    "        rs = requests.session()\n",
    "        res_subpage = rs.post(\"https://www.ptt.cc/ask/over18\", verify = False, data = payload)\n",
    "        res_subpage = rs.get(link, verify = False)\n",
    "        soup_subpage = BeautifulSoup(res_subpage.text, \"html.parser\")\n",
    "\n",
    "        #一篇文章的 作者、標題、時間、看板\n",
    "        header = soup_subpage.find_all(\"span\", \"article-meta-value\")\n",
    "        #print(header)\n",
    "        #print(header[0].text) #author\n",
    "        #print(header[1].text) #board\n",
    "        #print(header[2].text) #title\n",
    "        #print(header[3].text) #date\n",
    "        \n",
    "        try: #如果有「看板」的話\n",
    "            names[\"M_%s\" %article_id][\"Author\"] = header[0].text\n",
    "            names[\"M_%s\" %article_id][\"Board\"] = header[1].text\n",
    "            names[\"M_%s\" %article_id][\"Title\"] = header[2].text\n",
    "            names[\"M_%s\" %article_id][\"Date\"] = header[3].text\n",
    "        except:\n",
    "            try: #如果是回覆文章，沒有「看板」的話\n",
    "                names[\"M_%s\" %article_id][\"Author\"] = header[0].text\n",
    "                names[\"M_%s\" %article_id][\"Title\"] = header[1].text\n",
    "                names[\"M_%s\" %article_id][\"Date\"] = header[2].text\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # 內文\n",
    "        try:\n",
    "            main_text = soup_subpage.find(id = \"main-container\").text\n",
    "        except:\n",
    "            pass\n",
    "        #print(main_text)\n",
    "        # 把整個內容切割透過 \"-- \" 切割成2個陣列\n",
    "        pre_text = main_text.split('--')[0]\n",
    "        # 把每段文字 根據 '\\n' 切開\n",
    "        texts = pre_text.split('\\n')\n",
    "        # 如果你爬多篇你會發現 \n",
    "        contents = texts[2:]\n",
    "        # 內容\n",
    "        content = '\\n'.join(contents)\n",
    "        #print('內容：'+content)\n",
    "        names[\"M_%s\" %article_id][\"Content\"] = content\n",
    "\n",
    "        \n",
    "        #IP位址\n",
    "        try:\n",
    "            try:\n",
    "                IP_address = main_text.split('來自:')[1].split(\"\\n\")[0]\n",
    "                #print(IP_address)\n",
    "                names[\"M_%s\" %article_id][\"Full IP\"] = IP_address\n",
    "            except:\n",
    "                try:\n",
    "                    author = header[0].text\n",
    "                    authorID = author.split(sep = \" (\")[0]\n",
    "                    #print(main_text.split(sep = authorID)[2].split(sep = \"),\")[0].split()[0][1:])\n",
    "                    IP_address = main_text.split(sep = authorID)[2].split(sep = \"),\")[0].split()[0][1:]\n",
    "                    names[\"M_%s\" %article_id][\"Full IP\"] = IP_address\n",
    "                except:\n",
    "                    pass\n",
    "        except:\n",
    "            names[\"M_%s\" %article_id][\"Full IP\"] = \"Not Found\"\n",
    "        \n",
    "        #只有數字的IP\n",
    "        try:\n",
    "            IP_shorten = IP_address.split(sep = \" \")[1]\n",
    "            names[\"M_%s\" %article_id][\"IP\"] = IP_shorten\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # 推文\n",
    "        try:            \n",
    "            push = main_text.split(sep = \"html\")[1].split(sep = \"\\n\")[1:]\n",
    "            names[\"M_%s\" %article_id][\"Push\"] = push\n",
    "        except:\n",
    "            names[\"M_%s\" %article_id][\"Push\"] = \"No Push\"\n",
    "        \n",
    "        time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"title\">\n",
       "<a href=\"/bbs/Gossiping/M.1580085626.A.7F4.html\">[問卦] 賈伯斯死掉跟kobe死掉哪個比較震撼?</a>\n",
       "</div>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<span class=\"article-meta-value\">bluebluelan (白魚挺粗強貝柱連根立)</span>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "author = header[0].text\n",
    "authorID = author.split(sep = \" (\")[0]\n",
    "#print(main_text.split(sep = authorID)[2].split(sep = \"),\")[0].split()[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bluebluelan'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authorID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1040"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(earthquake_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'M10', 'M11', 'M12', 'M13', 'M14', 'M15', 'M16', 'M17', 'M18', 'M19', 'M20', 'M21', 'M22', 'M23', 'M24', 'M25', 'M26', 'M27', 'M28', 'M29', 'M30', 'M31', 'M32', 'M33', 'M34', 'M35', 'M36', 'M37', 'M38', 'M39', 'M40', 'M41', 'M42', 'M43', 'M44', 'M45', 'M46', 'M47', 'M48', 'M49', 'M50', 'M51', 'M52', 'M53', 'M54', 'M55', 'M56', 'M57', 'M58', 'M59', 'M60', 'M61', 'M62', 'M63', 'M64', 'M65', 'M66', 'M67', 'M68', 'M69', 'M70', 'M71', 'M72', 'M73', 'M74', 'M75', 'M76', 'M77', 'M78', 'M79', 'M80', 'M81', 'M82', 'M83', 'M84', 'M85', 'M86', 'M87', 'M88', 'M89', 'M90', 'M91', 'M92', 'M93', 'M94', 'M95', 'M96', 'M97', 'M98', 'M99', 'M100', 'M101', 'M102', 'M103', 'M104', 'M105', 'M106', 'M107', 'M108', 'M109', 'M110', 'M111', 'M112', 'M113', 'M114', 'M115', 'M116', 'M117', 'M118', 'M119', 'M120', 'M121', 'M122', 'M123', 'M124', 'M125', 'M126', 'M127', 'M128', 'M129', 'M130', 'M131', 'M132', 'M133', 'M134', 'M135', 'M136', 'M137', 'M138', 'M139', 'M140', 'M141', 'M142', 'M143', 'M144', 'M145', 'M146', 'M147', 'M148', 'M149', 'M150', 'M151', 'M152', 'M153', 'M154', 'M155', 'M156', 'M157', 'M158', 'M159', 'M160', 'M161', 'M162', 'M163', 'M164', 'M165', 'M166', 'M167', 'M168', 'M169', 'M170', 'M171', 'M172', 'M173', 'M174', 'M175', 'M176', 'M177', 'M178', 'M179', 'M180', 'M181', 'M182', 'M183', 'M184', 'M185', 'M186', 'M187', 'M188', 'M189', 'M190', 'M191', 'M192', 'M193', 'M194', 'M195', 'M196', 'M197', 'M198', 'M199', 'M200', 'M201', 'M202', 'M203', 'M204', 'M205', 'M206', 'M207', 'M208', 'M209', 'M210', 'M211', 'M212', 'M213', 'M214', 'M215', 'M216', 'M217', 'M218', 'M219', 'M220', 'M221', 'M222', 'M223', 'M224', 'M225', 'M226', 'M227', 'M228', 'M229', 'M230', 'M231', 'M232', 'M233', 'M234', 'M235', 'M236', 'M237', 'M238', 'M239', 'M240', 'M241', 'M242', 'M243', 'M244', 'M245', 'M246', 'M247', 'M248', 'M249', 'M250', 'M251', 'M252', 'M253', 'M254', 'M255', 'M256', 'M257', 'M258', 'M259', 'M260', 'M261', 'M262', 'M263', 'M264', 'M265', 'M266', 'M267', 'M268', 'M269', 'M270', 'M271', 'M272', 'M273', 'M274', 'M275', 'M276', 'M277', 'M278', 'M279', 'M280', 'M281', 'M282', 'M283', 'M284', 'M285', 'M286', 'M287', 'M288', 'M289', 'M290', 'M291', 'M292', 'M293', 'M294', 'M295', 'M296', 'M297', 'M298', 'M299', 'M300', 'M301', 'M302', 'M303', 'M304', 'M305', 'M306', 'M307', 'M308', 'M309', 'M310', 'M311', 'M312', 'M313', 'M314', 'M315', 'M316', 'M317', 'M318', 'M319', 'M320', 'M321', 'M322', 'M323', 'M324', 'M325', 'M326', 'M327', 'M328', 'M329', 'M330', 'M331', 'M332', 'M333', 'M334', 'M335', 'M336', 'M337', 'M338', 'M339', 'M340', 'M341', 'M342', 'M343', 'M344', 'M345', 'M346', 'M347', 'M348', 'M349', 'M350', 'M351', 'M352', 'M353', 'M354', 'M355', 'M356', 'M357', 'M358', 'M359', 'M360', 'M361', 'M362', 'M363', 'M364', 'M365', 'M366', 'M367', 'M368', 'M369', 'M370', 'M371', 'M372', 'M373', 'M374', 'M375', 'M376', 'M377', 'M378', 'M379', 'M380', 'M381', 'M382', 'M383', 'M384', 'M385', 'M386', 'M387', 'M388', 'M389', 'M390', 'M391', 'M392', 'M393', 'M394', 'M395', 'M396', 'M397', 'M398', 'M399', 'M400', 'M401', 'M402', 'M403', 'M404', 'M405', 'M406', 'M407', 'M408', 'M409', 'M410', 'M411', 'M412', 'M413', 'M414', 'M415', 'M416', 'M417', 'M418', 'M419', 'M420', 'M421', 'M422', 'M423', 'M424', 'M425', 'M426', 'M427', 'M428', 'M429', 'M430', 'M431', 'M432', 'M433', 'M434', 'M435', 'M436', 'M437', 'M438', 'M439', 'M440', 'M441', 'M442', 'M443', 'M444', 'M445', 'M446', 'M447', 'M448', 'M449', 'M450', 'M451', 'M452', 'M453', 'M454', 'M455', 'M456', 'M457', 'M458', 'M459', 'M460', 'M461', 'M462', 'M463', 'M464', 'M465', 'M466', 'M467', 'M468', 'M469', 'M470', 'M471', 'M472', 'M473', 'M474', 'M475', 'M476', 'M477', 'M478', 'M479', 'M480', 'M481', 'M482', 'M483', 'M484', 'M485', 'M486', 'M487', 'M488', 'M489', 'M490', 'M491', 'M492', 'M493', 'M494', 'M495', 'M496', 'M497', 'M498', 'M499', 'M500', 'M501', 'M502', 'M503', 'M504', 'M505', 'M506', 'M507', 'M508', 'M509', 'M510', 'M511', 'M512', 'M513', 'M514', 'M515', 'M516', 'M517', 'M518', 'M519', 'M520', 'M521', 'M522', 'M523', 'M524', 'M525', 'M526', 'M527', 'M528', 'M529', 'M530', 'M531', 'M532', 'M533', 'M534', 'M535', 'M536', 'M537', 'M538', 'M539', 'M540', 'M541', 'M542', 'M543', 'M544', 'M545', 'M546', 'M547', 'M548', 'M549', 'M550', 'M551', 'M552', 'M553', 'M554', 'M555', 'M556', 'M557', 'M558', 'M559', 'M560', 'M561', 'M562', 'M563', 'M564', 'M565', 'M566', 'M567', 'M568', 'M569', 'M570', 'M571', 'M572', 'M573', 'M574', 'M575', 'M576', 'M577', 'M578', 'M579', 'M580', 'M581', 'M582', 'M583', 'M584', 'M585', 'M586', 'M587', 'M588', 'M589', 'M590', 'M591', 'M592', 'M593', 'M594', 'M595', 'M596', 'M597', 'M598', 'M599', 'M600', 'M601', 'M602', 'M603', 'M604', 'M605', 'M606', 'M607', 'M608', 'M609', 'M610', 'M611', 'M612', 'M613', 'M614', 'M615', 'M616', 'M617', 'M618', 'M619', 'M620', 'M621', 'M622', 'M623', 'M624', 'M625', 'M626', 'M627', 'M628', 'M629', 'M630', 'M631', 'M632', 'M633', 'M634', 'M635', 'M636', 'M637', 'M638', 'M639', 'M640', 'M641', 'M642', 'M643', 'M644', 'M645', 'M646', 'M647', 'M648', 'M649', 'M650', 'M651', 'M652', 'M653', 'M654', 'M655', 'M656', 'M657', 'M658', 'M659', 'M660', 'M661', 'M662', 'M663', 'M664', 'M665', 'M666', 'M667', 'M668', 'M669', 'M670', 'M671', 'M672', 'M673', 'M674', 'M675', 'M676', 'M677', 'M678', 'M679', 'M680', 'M681', 'M682', 'M683', 'M684', 'M685', 'M686', 'M687', 'M688', 'M689', 'M690', 'M691', 'M692', 'M693', 'M694', 'M695', 'M696', 'M697', 'M698', 'M699', 'M700', 'M701', 'M702', 'M703', 'M704', 'M705', 'M706', 'M707', 'M708', 'M709', 'M710', 'M711', 'M712', 'M713', 'M714', 'M715', 'M716', 'M717', 'M718', 'M719', 'M720', 'M721', 'M722', 'M723', 'M724', 'M725', 'M726', 'M727', 'M728', 'M729', 'M730', 'M731', 'M732', 'M733', 'M734', 'M735', 'M736', 'M737', 'M738', 'M739', 'M740', 'M741', 'M742', 'M743', 'M744', 'M745', 'M746', 'M747', 'M748', 'M749', 'M750', 'M751', 'M752', 'M753', 'M754', 'M755', 'M756', 'M757', 'M758', 'M759', 'M760', 'M761', 'M762', 'M763', 'M764', 'M765', 'M766', 'M767', 'M768', 'M769', 'M770', 'M771', 'M772', 'M773', 'M774', 'M775', 'M776', 'M777', 'M778', 'M779', 'M780', 'M781', 'M782', 'M783', 'M784', 'M785', 'M786', 'M787', 'M788', 'M789', 'M790', 'M791', 'M792', 'M793', 'M794', 'M795', 'M796', 'M797', 'M798', 'M799', 'M800', 'M801', 'M802', 'M803', 'M804', 'M805', 'M806', 'M807', 'M808', 'M809', 'M810', 'M811', 'M812', 'M813', 'M814', 'M815', 'M816', 'M817', 'M818', 'M819', 'M820', 'M821', 'M822', 'M823', 'M824', 'M825', 'M826', 'M827', 'M828', 'M829', 'M830', 'M831', 'M832', 'M833', 'M834', 'M835', 'M836', 'M837', 'M838', 'M839', 'M840', 'M841', 'M842', 'M843', 'M844', 'M845', 'M846', 'M847', 'M848', 'M849', 'M850', 'M851', 'M852', 'M853', 'M854', 'M855', 'M856', 'M857', 'M858', 'M859', 'M860', 'M861', 'M862', 'M863', 'M864', 'M865', 'M866', 'M867', 'M868', 'M869', 'M870', 'M871', 'M872', 'M873', 'M874', 'M875', 'M876', 'M877', 'M878', 'M879', 'M880', 'M881', 'M882', 'M883', 'M884', 'M885', 'M886', 'M887', 'M888', 'M889', 'M890', 'M891', 'M892', 'M893', 'M894', 'M895', 'M896', 'M897', 'M898', 'M899', 'M900', 'M901', 'M902', 'M903', 'M904', 'M905', 'M906', 'M907', 'M908', 'M909', 'M910', 'M911', 'M912', 'M913', 'M914', 'M915', 'M916', 'M917', 'M918', 'M919', 'M920', 'M921', 'M922', 'M923', 'M924', 'M925', 'M926', 'M927', 'M928', 'M929', 'M930', 'M931', 'M932', 'M933', 'M934', 'M935', 'M936', 'M937', 'M938', 'M939', 'M940', 'M941', 'M942', 'M943', 'M944', 'M945', 'M946', 'M947', 'M948', 'M949', 'M950', 'M951', 'M952', 'M953', 'M954', 'M955', 'M956', 'M957', 'M958', 'M959', 'M960', 'M961', 'M962', 'M963', 'M964', 'M965', 'M966', 'M967', 'M968', 'M969', 'M970', 'M971', 'M972', 'M973', 'M974', 'M975', 'M976', 'M977', 'M978', 'M979', 'M980', 'M981', 'M982', 'M983', 'M984', 'M985', 'M986', 'M987', 'M988', 'M989', 'M990', 'M991', 'M992', 'M993', 'M994', 'M995', 'M996', 'M997', 'M998', 'M999', 'M1000', 'M1001', 'M1002', 'M1003', 'M1004', 'M1005', 'M1006', 'M1007', 'M1008', 'M1009', 'M1010', 'M1011', 'M1012', 'M1013', 'M1014', 'M1015', 'M1016', 'M1017', 'M1018', 'M1019', 'M1020', 'M1021', 'M1022', 'M1023', 'M1024', 'M1025', 'M1026', 'M1027', 'M1028', 'M1029', 'M1030', 'M1031', 'M1032', 'M1033', 'M1034', 'M1035', 'M1036', 'M1037', 'M1038', 'M1039', 'M1040'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earthquake_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Author': 'musashi4735 (武藏)',\n",
       " 'Board': 'Gossiping',\n",
       " 'Title': '[爆卦] 地震',\n",
       " 'Date': 'Wed Jun 17 07:18:17 2020',\n",
       " 'Content': '地震!!中壢\\n\\n',\n",
       " 'Full IP': ' 114.32.216.39 (臺灣)',\n",
       " 'IP': '114.32.216.39',\n",
       " 'Push': ['※ 編輯: musashi4735 (114.32.216.39 臺灣), 06/17/2020 07:18:51',\n",
       "  '',\n",
       "  '',\n",
       "  '本網站已依台灣網站內容分級規定處理。此區域為限制級，未滿十八歲者不得瀏覽。',\n",
       "  '']}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earthquake_dict[\"M3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'73.63.220.93'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IP_shorten = IP_address.split(sep = \" \")[1]\n",
    "IP_shorten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "np.save(\"earthquake_dict_0617地震\", earthquake_dict)\n",
    "\n",
    "# Load\n",
    "#read_dictionary = np.load('my_file.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Author': 'wurara22 (烏拉餓餓)',\n",
       " 'Board': 'Gossiping',\n",
       " 'Title': '[問卦] 今天早上的地震很大嗎?',\n",
       " 'Date': 'Sun Jun 14 22:51:41 2020',\n",
       " 'Content': \"大家晚安\\n如題\\n\\n剛剛看新聞才看到今天早上4:19有地震\\n還說是全台有感 規模6.0\\n\\n可是魯妹根本沒感覺...\\n\\n之前半夜有地震的話我都會醒來\\n\\n\\n昨晚跟平常不同的是有戴耳塞睡覺\\n可能因為耳塞沒聽到家具震動的聲音所以沒注意到地震??\\n\\n有沒有八卦?0'_'0\\n\\n\",\n",
       " 'Full IP': ' 118.166.126.3 (臺灣)',\n",
       " 'IP': '118.166.126.3',\n",
       " 'Push': ['→ akila08539: 灰塵貓有感覺嗎 06/14 22:52',\n",
       "  '今天早上的因為我沒感覺所以也不知道灰塵貓怎樣',\n",
       "  '不過之前有地震的時候她是沒什麼反應，頂多抬頭四處觀望',\n",
       "  '學姐貓是完全睡死zzz',\n",
       "  '',\n",
       "  '我是有在床頭櫃放一個洗衣袋啦，有需要把她包進去一起逃',\n",
       "  '噓 jenn8588: 沒人關心 06/14 22:52',\n",
       "  '推 XXXXLINDA: 還好 06/14 22:52',\n",
       "  '→ saobox: 意淫地震 8888888 06/14 22:52',\n",
       "  '推 purplebfly: 事實上我是被手機吵醒,不是被地震吵醒的 06/14 22:52',\n",
       "  '這也很奇怪',\n",
       "  '之前我會收到警報，但是今天早上沒有',\n",
       "  '推 wade6510: 最大三級  因為在海裡 算中層深度 06/14 22:52',\n",
       "  '※ 編輯: wurara22 (118.166.126.3 臺灣), 06/14/2020 22:56:08',\n",
       "  '推 Xhocer: 我睡得跟死豬依樣 06/14 22:52',\n",
       "  '推 dWoWb: 是喔...早上起來看八卦版也沒特別看到地震 LUL 06/14 22:54',\n",
       "  '因為新聞寫是今年最強，但是我卻睡到翻，所以想知道到底是不是今年最強',\n",
       "  '※ 編輯: wurara22 (118.166.126.3 臺灣), 06/14/2020 22:57:05',\n",
       "  '→ SFGiants: 美國無感 XD 06/14 23:01',\n",
       "  '推 ted01234567: 有感 但是太累了繼續睡 06/14 23:43',\n",
       "  '→ a3221715: 睡不好 不知道是不是因為地震 06/14 23:55',\n",
       "  '→ a3221715: 但睡死無感 06/14 23:55',\n",
       "  '推 cuteSquirrel: 很晃 06/15 00:05',\n",
       "  '推 tuutu: 有感 我的鸚鵡被驚醒亂飛 後來我把他抓來一起睡 06/15 01:53',\n",
       "  '',\n",
       "  '',\n",
       "  '本網站已依台灣網站內容分級規定處理。此區域為限制級，未滿十八歲者不得瀏覽。',\n",
       "  '']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earthquake_dict[\"M20\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
